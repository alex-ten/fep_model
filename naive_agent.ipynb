{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build environment and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, representation, actions, persistent=True) -> None:\n",
    "        self.representation = representation\n",
    "        self.actions = actions\n",
    "        self.transitions = {'null': ([self], np.ones(1))} if persistent else {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'State {self.representation}'\n",
    "\n",
    "\n",
    "class MonsterTaskEnv(object):\n",
    "    def __init__(self, states, agent, init_state_ind=0) -> None:\n",
    "        self.states = states\n",
    "        self.state = states[init_state_ind]\n",
    "        self.agent = agent\n",
    "        self.agent.env = self\n",
    "\n",
    "    def transition(self, action):\n",
    "        new_states, probas = self.state.transitions[action]\n",
    "        new_state = np.random.choice(new_states, size=1, p=probas)[0]\n",
    "        self.state = new_state\n",
    "        return new_state\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, actions, env=None) -> None:\n",
    "        self.env = env\n",
    "        self.actions = actions\n",
    "        self.policy = None\n",
    "\n",
    "    def generate_responses(self, state):\n",
    "        # Responses\n",
    "        return list(set(state.actions).intersection(self.actions))\n",
    "\n",
    "    def generate_predictions(self, state, responses):\n",
    "        # Predictions should be a probability distribution over next states\n",
    "        predictions = {}\n",
    "        for response in responses:\n",
    "            predictions[response] = (state.transitions[response])\n",
    "        return None\n",
    "\n",
    "    def choose_response(self, responses, predictions):\n",
    "        # For now, ignore predictions and choose randomly\n",
    "        if len(responses) > 1:\n",
    "            return responses[randint(0, len(responses) - 1)]\n",
    "        else:\n",
    "            return responses[0]\n",
    "\n",
    "    def learn(self, *args):\n",
    "        # Here, the agent should update its models or response tendencies.\n",
    "        pass\n",
    "\n",
    "    def step(self, auto):\n",
    "        if not auto:\n",
    "            usr_inp = input('Press enter to continue')\n",
    "            if usr_inp == 'exit':\n",
    "                return False\n",
    "        state = self.env.state\n",
    "        responses = self.generate_responses(state)\n",
    "        predictions = self.generate_predictions(state, responses)\n",
    "        choice = self.choose_response(responses, predictions)\n",
    "        new_state = self.env.transition(choice)\n",
    "        self.learn(state, responses, predictions, choice, new_state)\n",
    "        return True\n",
    "\n",
    "    def go(self, timesteps, auto=True):\n",
    "        for t in range(timesteps):\n",
    "            print(f'Step {t}')\n",
    "            if not self.step(auto):\n",
    "                break\n",
    "\n",
    "    def give_control(self, max_timesteps):\n",
    "        for i in range(max_timesteps):\n",
    "            state = self.env.state\n",
    "            responses = self.generate_responses(state)\n",
    "            \n",
    "            response = '...'\n",
    "            if state.representation[0] == 1:\n",
    "                print(f'Choose monster family {responses}')\n",
    "            elif state.representation[0] == 2:\n",
    "                print(f'What does this monster like to eat? {responses}\\nFamily {state.representation[1]}, {state.representation[2:4]}')\n",
    "            elif state.representation[0] == 3:\n",
    "                if state.representation[-1] == 1:\n",
    "                    print('Correct!')\n",
    "                if state.representation[-1] == 2:\n",
    "                    print('Incorrect!')\n",
    "                response = 'null'\n",
    "            while response not in responses:\n",
    "                response = input()\n",
    "                if response == 'exit':\n",
    "                    break\n",
    "            self.env.transition(response)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three kinds of states in the environment, each characterized by a unique, yet semantically meaningful representation, and a set of actions available. Each state is represented by a 5-element vector where each element encodes some qualitative information about the state.\n",
    "\n",
    "For example, the choice state, represented as a vector `[1, 0, 0, 0, 0]`. The `1` in the first position indicates that this is the choice stage and 0 values in other positions indicate that the features characterizing other states are absent in this state. There are 4 actions available in this state: `a1`, `a2`, `a3`, and `a4`. These actions correspond to family choices and each of them leads to a different set of states.\n",
    "\n",
    "For example, action `a1` leads to one of the states that have `2` in the first position, and `1` in the second position, e.g., `[2, 1, 3, 5, 0]`. The leading `2` encodes the fact that the state is a guess state (not a choice state), and values in the 3rd and 4th positions encode the features of a particular stimulus (monster) presented to the agent. Each of the guess states have two actions whereby all guess states presenting monsters from the same family have the same pair of actions (the same food choices). \n",
    "\n",
    "The last example state `[2, 1, 3, 5, 0]` has actions `f11` and `f12` corresponding to two food choices. Each action, depending on the rule, leads to the third type of state, the feedback state, which can be either `[3, 0, 0, 0, 1]` for a positive feedback or `[3, 0, 0, 0, 1]` for the negative feedback. As you can see, the states differ semantically, but there is nothing inherently positive or negative about any particular state. The feedback states both have a null action `null` that inadvertently leads to the choice state. The `null` action is present in all other states, but in these cases, it results in an \"autotransition\" so that the state does not change.\n",
    "\n",
    "To simplify things, we can imagine that instead of the feedback states being evaluated by the agent, the environment emits a reward for the agent to process (how it is usually done in RL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xnor(a, b):\n",
    "    return (a and b) or (not a and not b)\n",
    "\n",
    "\n",
    "states = []\n",
    "\n",
    "states += [\n",
    "    State(representation=np.array([1, 0, 0, 0, 0]), actions=['a1', 'a2', 'a3', 'a4', 'null']),\n",
    "    State(representation=np.array([3, 0, 0, 0, 1]), actions=['null'], persistent=False),\n",
    "    State(representation=np.array([3, 0, 0, 0, 2]), actions=['null'], persistent=False)\n",
    "]\n",
    "\n",
    "for i in range(1, 7):\n",
    "    states.append(\n",
    "        State(representation=np.array([2, 1, i, 3, 0]), actions=['f11', 'f12', 'null']),\n",
    "    )\n",
    "\n",
    "for i in range(2, 5):\n",
    "    actions = [f'f{i}1', f'f{i}2', 'null']\n",
    "    for j in range(1, 7):\n",
    "        for k in range(1, 7):\n",
    "            states.append(\n",
    "                State(representation=np.array([2, i, j, k, 0]), actions=actions),\n",
    "            )\n",
    "\n",
    "# Transitions from choice state\n",
    "for act in states[0].actions[:-1]:\n",
    "    to_states = [state for state in states[3:] if state.representation[1] == int(act[-1])]\n",
    "    states[0].transitions[act] = (to_states, np.ones(len(to_states))/len(to_states))\n",
    "\n",
    "# Transitions from feedback state\n",
    "states[1].transitions['null'] = ([states[0]], np.ones(1))\n",
    "states[2].transitions['null'] = ([states[0]], np.ones(1))\n",
    "\n",
    "# Transitions from guess states\n",
    "for state in states[3:]:\n",
    "    # 1d1 rule\n",
    "    if state.representation[1] == 1:\n",
    "        rule = {'f11': states[1], 'f12': states[2]} if state.representation[2] < 4 else {'f11': states[2], 'f12': states[1]}\n",
    "        state.transitions['f11'] = ([rule['f11']], np.ones(1))\n",
    "        state.transitions['f12'] = ([rule['f12']], np.ones(1))\n",
    "    # 2d1 rule\n",
    "    elif state.representation[1] == 2:\n",
    "        rule = {'f21': states[1], 'f22': states[2]} if state.representation[2] < 4 else {'f21': states[2], 'f22': states[1]}\n",
    "        state.transitions['f21'] = ([rule['f21']], np.ones(1))\n",
    "        state.transitions['f22'] = ([rule['f22']], np.ones(1))\n",
    "    # 2d2 rule\n",
    "    elif state.representation[1] == 3:\n",
    "        rule = {'f31': states[1], 'f32': states[2]} if xnor(state.representation[2] < 4, state.representation[3] < 4) else {'f31': states[2], 'f32': states[1]}\n",
    "        state.transitions['f31'] = ([rule['f31']], np.ones(1))\n",
    "        state.transitions['f32'] = ([rule['f32']], np.ones(1))\n",
    "    elif state.representation[1] == 4:\n",
    "        state.transitions['f41'] = ([states[1], states[2]], np.ones(2)/2)\n",
    "        state.transitions['f42'] = ([states[1], states[2]], np.ones(2)/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose monster family ['a2', 'null', 'a1', 'a4', 'a3']\n",
      "What does this monster like to eat? ['null', 'f12', 'f11']\n",
      "Family 1, [6 3]\n",
      "Incorrect!\n",
      "Choose monster family ['a2', 'null', 'a1', 'a4', 'a3']\n",
      "What does this monster like to eat? ['null', 'f12', 'f11']\n",
      "Family 1, [3 3]\n",
      "Correct!\n",
      "Choose monster family ['a2', 'null', 'a1', 'a4', 'a3']\n",
      "What does this monster like to eat? ['null', 'f12', 'f11']\n",
      "Family 1, [2 3]\n",
      "Correct!\n",
      "Choose monster family ['a2', 'null', 'a1', 'a4', 'a3']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'exit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m env \u001b[39m=\u001b[39m MonsterTaskEnv(states\u001b[39m=\u001b[39mstates, agent\u001b[39m=\u001b[39magent)\n\u001b[1;32m      3\u001b[0m \u001b[39m# agent.go(50)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m agent\u001b[39m.\u001b[39;49mgive_control(\u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [31], line 82\u001b[0m, in \u001b[0;36mAgent.give_control\u001b[0;34m(self, max_timesteps)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     81\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mtransition(response)\n",
      "Cell \u001b[0;32mIn [31], line 19\u001b[0m, in \u001b[0;36mMonsterTaskEnv.transition\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransition\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> 19\u001b[0m     new_states, probas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mtransitions[action]\n\u001b[1;32m     20\u001b[0m     new_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(new_states, size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, p\u001b[39m=\u001b[39mprobas)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m new_state\n",
      "\u001b[0;31mKeyError\u001b[0m: 'exit'"
     ]
    }
   ],
   "source": [
    "agent = Agent(actions=['null', 'a1','a2','a3','a4','f11','f12','f21','f22','f31','f32','f41','f42'])\n",
    "env = MonsterTaskEnv(states=states, agent=agent)\n",
    "# agent.go(50)\n",
    "agent.give_control(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0d92c0f48e14e92ccab483581ae68e0ce8721711bfeacc013e13767dafa3a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
