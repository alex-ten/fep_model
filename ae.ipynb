{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image, ImageOps\n",
    "import csv, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AutoEncoder model\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, hidden_size=50, act_hist={}, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=hidden_size\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=hidden_size, out_features=hidden_size\n",
    "        )\n",
    "        self.decoder_hidden_layer = nn.Linear(\n",
    "            in_features=hidden_size, out_features=hidden_size\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=hidden_size, out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "        self.state = {}\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.sigmoid(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.sigmoid(code)\n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.sigmoid(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.sigmoid(activation)\n",
    "        return reconstructed\n",
    "\n",
    "    # this method runs on every forward pass if attach_hook if attach_hook is called beforehand\n",
    "    def get_activation(self, module_name):\n",
    "        def hook(model, input, output):\n",
    "            self.state[module_name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def attach_hooks(self, module_names):\n",
    "        for name in module_names:\n",
    "            layer = getattr(self, name)\n",
    "            layer.register_forward_hook(self.get_activation(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/10000, loss = 0.176268\n",
      "epoch : 501/10000, loss = 0.014145\n",
      "epoch : 1001/10000, loss = 0.002200\n",
      "epoch : 1501/10000, loss = 0.001875\n",
      "epoch : 2001/10000, loss = 0.001520\n",
      "epoch : 2501/10000, loss = 0.001364\n",
      "epoch : 3001/10000, loss = 0.001255\n",
      "epoch : 3501/10000, loss = 0.001153\n",
      "epoch : 4001/10000, loss = 0.001138\n",
      "epoch : 4501/10000, loss = 0.001071\n",
      "epoch : 5001/10000, loss = 0.001037\n",
      "epoch : 5501/10000, loss = 0.001000\n",
      "epoch : 6001/10000, loss = 0.000953\n",
      "epoch : 6501/10000, loss = 0.000919\n",
      "epoch : 7001/10000, loss = 0.000899\n",
      "epoch : 7501/10000, loss = 0.000889\n",
      "epoch : 8001/10000, loss = 0.000877\n",
      "epoch : 8501/10000, loss = 0.000863\n",
      "epoch : 9001/10000, loss = 0.000957\n",
      "epoch : 9501/10000, loss = 0.000831\n",
      "epoch : 10000/10000, loss = 0.000799\n"
     ]
    }
   ],
   "source": [
    "FAM = 'f4'\n",
    "HLS = 20\n",
    "\n",
    "# Loader for input data\n",
    "def get_loader(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.RandomRotation(20),\n",
    "        # transforms.RandomResizedCrop(128),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()])\n",
    "    dataset = datasets.ImageFolder(f'monsters/{FAM}', transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a model from `AE` autoencoder class and\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(hidden_size=HLS, input_shape=64*64).to(device)\n",
    "# model.load_state_dict(torch.load(f'model_data/f4/checkpoint0001'))\n",
    "# model.eval()\n",
    "model.attach_hooks(['encoder_hidden_layer', 'encoder_output_layer', 'decoder_hidden_layer', 'decoder_output_layer'])\n",
    "\n",
    "# Create an optimizer object:\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define mean-squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training params\n",
    "epochs = 10000\n",
    "checkpoint = 500\n",
    "batch_size = 1\n",
    "\n",
    "# Data management routines\n",
    "loader = get_loader(batch_size)\n",
    "loss_hist = []\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for i_batch, (batch_features, _) in enumerate(loader):\n",
    "        # Eeshape mini-batch data to [N, 50*50] matrix\n",
    "        # Load it to the active device\n",
    "        batch_features = batch_features.view(-1, 64*64).to(device)\n",
    "        \n",
    "        # Reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run forward pass (and run hooks)\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # Compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # Compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # Compute the epoch training loss\n",
    "    loss = loss / len(loader)\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    # display the epoch training loss\n",
    "    if (epoch % checkpoint == 0) or (epoch==epochs-1):\n",
    "        torch.save(model.state_dict(), f'model_data{HLS}/{FAM}/checkpoint{epoch+1:04d}')\n",
    "        print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0d92c0f48e14e92ccab483581ae68e0ce8721711bfeacc013e13767dafa3a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
